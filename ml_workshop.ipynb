{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=00008b>WWU CS Mentors Machine Learning Workshop</font>\n",
    "#### _By Robin Cosbey and Josh Myers-Dean_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What we will accomplish in this exercise \n",
    "- Learn how to read in a CSV and do some exploratory data analysis\n",
    "- Build a decision tree model from scratch\n",
    "- Evaluate our model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis\n",
    "First we will read in a csv file containing information about different species of iris flowers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Pandas is a great library for data manipulation and visualizations,\n",
    "here we read in the csv and represent it as a dataframe (df). I like to think\n",
    "of these as fancy dictionaries.\n",
    "'''\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.read_csv(\"iris.csv\")\n",
    "df.head() # allows us to peak at the first 5 rows of data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our data has 4 different features we can use and seem to be labelled by what class of iris they are. <br />\n",
    "Now that we've got a sneak peek of our data, let's try and see what else we can find out about it before we move on to our model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What is the shape of our data?\n",
    "print(\"Dataframe Shape: \", df.shape,\"\\n\")\n",
    "\n",
    "# Can we summarize our data?\n",
    "print(\"Summary Statistics:\\n\\n\", df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awesome! So now we have some useful information about the dataset we are working with. We can think of our dataset as a 150x5 matrix, or simply view it as a CSV with 150 entries with each entry having 5 features.\n",
    "<br /> \n",
    "<br />\n",
    "The summary statistics give us a view into how the data is distributed, let's take a more visual look at our data.\n",
    "You can get pretty creative in your EDA visualizations, but for now we will create a simple boxplot showing our summary statistics using the library matplotlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Can we plot the statistics we found above?\n",
    "ax = df.boxplot(grid=False)\n",
    "ax.set_title(\"Summary Statistics for Iris Dataset\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a peek at the overall statistics our data, but what we are really interested in is how each class differs from each other, so let's visualize that as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Here we create a few different dataframes, each representing a specific class. unique gives us each individual class\n",
    "and then we create a seperate dataframe for each class to be plotted as a boxplot.\n",
    "'''\n",
    "\n",
    "# There are 3 classes so we want one row of 3 columns \n",
    "fig, axes = plt.subplots(nrows=1, ncols=len(df['class'].unique()))\n",
    "for i,cls in enumerate(df['class'].unique()):\n",
    "    cls_df = df[df['class'] == cls]\n",
    "    ax = cls_df.boxplot(ax=axes[i], grid=False, rot=30)\n",
    "    ax.set_title(\"Summary Statistics for class {}\".format(cls))\n",
    "'''\n",
    "this is straight from the matplotlib documentation to make subplots a \n",
    "bit prettier, I adjusted the 'right' parameter\n",
    "'''\n",
    "plt.subplots_adjust(\n",
    "    left  = 0.125,  # the left side of the subplots of the figure\n",
    "    right = 2.9,    # the right side of the subplots of the figure\n",
    "    bottom = 0.1,   # the bottom of the subplots of the figure\n",
    "    top = 0.9,      # the top of the subplots of the figure\n",
    "    wspace = 0.2,   # the amount of width reserved for blank space between subplots\n",
    "    hspace = 0.2\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we talked about, sometimes data can be biased so let's make sure that we have a relatively even distribution of classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many data points are in each class?\n",
    "# What can we say about the distribution of the data?\n",
    "df['class'].value_counts().plot.bar(rot=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Excellent! Our 3 classes are balanced quite well. We did this by plotting a value counts of of the \"class\" column from our dataframe, represented as a bar plot.\n",
    "\n",
    "<br />\n",
    "Also notice how we didn't have to call a matplotlib function explicitly? That's because with pandas you can call numpy (numerical python) and matplotlib functions from within your current dataframe object! (Also note that the image was displayed without the plt.show() call, this is a IPython notebook specific thing)\n",
    "\n",
    "<br />\n",
    "<br />\n",
    "Now the only problem is that our class values are not numerical values! This will cause our classifier to spit out an error so let's treat each class as a a numerical value using label encoding. (Note label encoding does have it's pitfalls but will work for our use case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(df['class'])\n",
    "df['class'] = le.transform(df['class'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building Our Model\n",
    "Now that we have had a good peak at our data, let's start constructing a model! We want to be able to predict a category, which is a discrete value, so let's go with a decision tree!\n",
    "\n",
    "Technical | Non-Technical \n",
    "- | - \n",
    "![alt text](dt.png \"A picture of a decision tree\") | ![alt text](decision.tree.png \"A picture of a decision tree\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A few things before we start\n",
    "\n",
    "### Loss and gain functions\n",
    "#### Loss\n",
    "- Entropy can be thought as the measure of randomness in the data. The higher the entropy the harder it is to draw conclusions about the data, so we want to __minimize our loss function__.\n",
    "- Entropy is represented as $E(S)=\\sum_{i=1}^{c}-p_ilog_2(p_i)$ where $p_i$ is the frequentist probability of class 'i' and $c$ is the number of classes\n",
    "- If the data is homogeneous then entropy will be zero, if the data is equally divided across all classes we will start with an entropy of 1\n",
    "- In our case, since we have an even number of classes our entropy is $-\\frac{1}{3}log_2(\\frac{1}{3})-\\frac{1}{3}log_2(\\frac{1}{3})-\\frac{1}{3}log_2(\\frac{1}{3}) \\approx 1$\n",
    "- Our decision tree results in a leaf node whenever entropy is 0 (when the algoirthm decides on a class), if a node has entropy $> 0$, the node will require further splitting. So in essence, entropy controls how our tree decides to split.\n",
    "\n",
    "#### Gain\n",
    "- Information gain tells us how important a given attribute of a feature vector is. (i.e. Sepal Width)\n",
    "- We want to __maximize gain__ in order to find the most useful feature vector to discriminate between classes.\n",
    "- Information gain is represented as $IG(parent, children)=E(parent)-avg(E(children))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting vs Prediction\n",
    "![alt text](three-fits.png \"Examples of fitting\")\n",
    "\n",
    "#### Fit\n",
    "In supervised learning, fitting is how we \"train\" our model to associate data with given labels. In essence, fitting a model is measuring how well the model generalizes to similiar data that the model was trained on. \n",
    "\n",
    "#### Predict\n",
    "Prediction comes into play when we want to see how well our model trained: a good metric for a balanced dataset would be accuracy. We can measure accuracy by feeding our model unlabelled data and then compare our models label outputs to the ground truth labels, the more our model gets right, the higher the accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Constructing the 2 different node classes__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "''' \n",
    "We will be building our model using pure numpy so we only need this import for now \n",
    "Here we build the interior and leaf nodes for our tree\n",
    "'''\n",
    "\n",
    "class Node:\n",
    "    def __init__(self, left, right, rule):\n",
    "        '''\n",
    "        Create a node for our tree\n",
    "        '''\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.feature = rule[0]\n",
    "        self.threshold = rule[1]\n",
    "\n",
    "\n",
    "class Leaf:\n",
    "    def __init__(self, value):\n",
    "        \"\"\"\n",
    "        `value` is an array of class probabilities if classifier is True, else\n",
    "        the mean of the region and since we want to classify, this will be true.\n",
    "        \"\"\"\n",
    "        self.value = value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Building the tree__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Here is where we will be building our actual model\n",
    "\"\"\"\n",
    "class DecisionTree:\n",
    "    # init is the initial values of the object when constructed\n",
    "    def __init__(\n",
    "        self,\n",
    "        classifier=True,\n",
    "        max_depth=None,\n",
    "        n_feats=None,\n",
    "        criterion=\"entropy\",\n",
    "        seed=None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        A decision tree model for regression and classification problems.\n",
    "        Parameters\n",
    "        ----------\n",
    "        classifier : bool\n",
    "            Whether to treat target values as categorical (classifier =\n",
    "            True) or continuous (classifier = False). Default is True.\n",
    "        max_depth: int or None\n",
    "            The depth at which to stop growing the tree. If None, grow the tree\n",
    "            until all leaves are pure. Default is None.\n",
    "        n_feats : int\n",
    "            Specifies the number of features to sample on each split. If None,\n",
    "            use all features on each split. Default is None.\n",
    "        criterion : {'mse', 'entropy', 'gini'}\n",
    "            The error criterion to use when calculating splits. When\n",
    "            `classifier` is False, valid entries are {'mse'}. When `classifier`\n",
    "            is True, valid entries are {'entropy', 'gini'}. Default is\n",
    "            'entropy'.\n",
    "        seed : int or None\n",
    "            Seed for the random number generator. Default is None.\n",
    "        \"\"\"\n",
    "        if seed:\n",
    "            np.random.seed(seed)\n",
    "        self.depth = 0\n",
    "        self.root = None\n",
    "\n",
    "        self.n_feats = n_feats\n",
    "        self.criterion = criterion\n",
    "        self.classifier = classifier\n",
    "        self.max_depth = max_depth if max_depth else np.inf\n",
    "\n",
    "        if not classifier and criterion in [\"gini\", \"entropy\"]:\n",
    "            raise ValueError(\n",
    "                \"{} is a valid criterion only when classifier = True.\".format(criterion)\n",
    "            )\n",
    "        if classifier and criterion == \"mse\":\n",
    "            raise ValueError(\"`mse` is a valid criterion only when classifier = False.\")\n",
    "\n",
    "            \n",
    "    \"\"\"\n",
    "    Here is where we train our model to recognize similiar data to the data that we supplied\n",
    "    \"\"\"\n",
    "    def fit(self, X, Y):\n",
    "        \"\"\"\n",
    "        Fit a binary decision tree to a dataset.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : :py:class:`ndarray <numpy.ndarray>` of shape `(N, M)`\n",
    "            The training data of `N` examples, each with `M` features, for us\n",
    "            this is (150,5) as seen before.\n",
    "            \n",
    "        Y : :py:class:`ndarray <numpy.ndarray>` of shape `(N,)`\n",
    "            An array of integer class labels for each example in `X` if\n",
    "            self.classifier = True, otherwise the set of target values for\n",
    "            each example in `X`.\n",
    "        \"\"\"\n",
    "        self.n_classes = max(Y) + 1 if self.classifier else None # account for 0 indexing\n",
    "        self.n_feats = X.shape[1] if not self.n_feats else min(self.n_feats, X.shape[1])\n",
    "        self.root = self._grow(X, Y)\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Use the trained decision tree to classify or predict the examples in `X`.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : :py:class:`ndarray <numpy.ndarray>` of shape `(N, M)`\n",
    "            The training data of `N` examples, each with `M` features\n",
    "        Returns\n",
    "        -------\n",
    "        preds : :py:class:`ndarray <numpy.ndarray>` of shape `(N,)`\n",
    "            The integer class labels predicted for each example in `X` if\n",
    "            self.classifier = True, otherwise the predicted target values.\n",
    "        \"\"\"\n",
    "        # returns the highest class probability\n",
    "        return np.array([self._traverse(x, self.root) for x in X])\n",
    "\n",
    "    def predict_class_probs(self, X):\n",
    "        \"\"\"\n",
    "        Use the trained decision tree to return the class probabilities for the\n",
    "        examples in `X`.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : :py:class:`ndarray <numpy.ndarray>` of shape `(N, M)`\n",
    "            The training data of `N` examples, each with `M` features\n",
    "        Returns\n",
    "        -------\n",
    "        preds : :py:class:`ndarray <numpy.ndarray>` of shape `(N, n_classes)`\n",
    "            The class probabilities predicted for each example in `X`.\n",
    "        \"\"\"\n",
    "        assert self.classifier, \"predict_class_probs undefined for classifier = False\"\n",
    "        # traverse the tree for each iris in the set to find out the class prob\n",
    "        return np.array([self._traverse(x, self.root, prob=True) for x in X])\n",
    "    \n",
    "    \n",
    "    # create new branches/leaves\n",
    "    def _grow(self, X, Y):\n",
    "        # if all labels are the same, return a leaf\n",
    "        if len(np.unique(Y)) == 1:\n",
    "            if self.classifier:\n",
    "                prob = np.zeros(self.n_classes) # create a 1x1 array\n",
    "                prob[Y[0]] = 1.0 # only one class so can only predict one class\n",
    "            return Leaf(prob) if self.classifier else Leaf(Y[0])\n",
    "\n",
    "        \n",
    "        # if we have reached max_depth, return a leaf\n",
    "        if self.depth >= self.max_depth:\n",
    "            v = np.mean(Y, axis=0)\n",
    "            if self.classifier:\n",
    "                v = np.bincount(Y, minlength=self.n_classes) / len(Y) \n",
    "                '''\n",
    "                 num occurences of each class in Y divided by the length of Y\n",
    "                '''\n",
    "            return Leaf(v)\n",
    "\n",
    "        n, m = X.shape # rows and cols of X\n",
    "        self.depth += 1\n",
    "        feat_idxs = np.random.choice(m, self.n_feats, replace=False)\n",
    "        ''' \n",
    "        Here we pick n_feats random numbers from numbers in range(m), this is where a \n",
    "        seed would have an impact\n",
    "        '''\n",
    "\n",
    "        # greedily select the best split according to `criterion`\n",
    "        feat, thresh = self._segment(X, Y, feat_idxs)\n",
    "        l = np.argwhere(X[:, feat] <= thresh).flatten()\n",
    "        r = np.argwhere(X[:, feat] > thresh).flatten()\n",
    "        # grow the children that result from the split, Recursion!!!\n",
    "        left = self._grow(X[l, :], Y[l])\n",
    "        right = self._grow(X[r, :], Y[r])\n",
    "        return Node(left, right, (feat, thresh))\n",
    "\n",
    "    \n",
    "    '''\n",
    "    These next 2 functions are a little wild but what is important to know is that they are used to\n",
    "    calculate the \"rule\" of the split in which the tree will branch. \n",
    "    Rules generally have the form, if condition1 and condition2 and condition3 then outcome.\n",
    "    '''\n",
    "    def _segment(self, X, Y, feat_idxs):\n",
    "        \"\"\"\n",
    "        Find the optimal split rule (feature index and split threshold) for the\n",
    "        data according to `self.criterion`.\n",
    "        \"\"\"\n",
    "        best_gain = -np.inf\n",
    "        split_idx, split_thresh = None, None\n",
    "        for i in feat_idxs:\n",
    "            vals = X[:, i]\n",
    "            levels = np.unique(vals)\n",
    "            thresholds = (levels[:-1] + levels[1:]) / 2 if len(levels) > 1 else levels\n",
    "            gains = np.array([self._information_gain(Y, t, vals) for t in thresholds])\n",
    "\n",
    "            if gains.max() > best_gain:\n",
    "                split_idx = i\n",
    "                best_gain = gains.max()\n",
    "                split_thresh = thresholds[gains.argmax()]\n",
    "\n",
    "        return split_idx, split_thresh\n",
    "    \n",
    "    # we want to maximize this\n",
    "    def _information_gain(self, Y, split_thresh, feat_values):\n",
    "        \"\"\"\n",
    "        Compute the information gain associated with a given split.\n",
    "        IG(split) = loss(parent) - weighted_avg[loss(left_child), loss(right_child)]\n",
    "        \"\"\"\n",
    "        if self.criterion == \"entropy\":\n",
    "            loss = entropy\n",
    "        elif self.criterion == \"gini\":\n",
    "            loss = gini\n",
    "        elif self.criterion == \"mse\":\n",
    "            loss = mse\n",
    "\n",
    "        parent_loss = loss(Y)\n",
    "\n",
    "        # generate split\n",
    "        left = np.argwhere(feat_values <= split_thresh).flatten()\n",
    "        right = np.argwhere(feat_values > split_thresh).flatten()\n",
    "        \n",
    "        # leaf node\n",
    "        if len(left) == 0 or len(right) == 0:\n",
    "            return 0\n",
    "\n",
    "        # compute the weighted avg. of the loss for the children\n",
    "        n = len(Y)\n",
    "        n_l, n_r = len(left), len(right)\n",
    "        e_l, e_r = loss(Y[left]), loss(Y[right])\n",
    "        child_loss = (n_l / n) * e_l + (n_r / n) * e_r\n",
    "\n",
    "        # information gain is difference in loss before vs. after split\n",
    "        ig = parent_loss - child_loss\n",
    "        return ig\n",
    "\n",
    "    '''\n",
    "    Here we traverse the the tree to find the highest probability\n",
    "    of a node being a part of a specific class\n",
    "    '''\n",
    "    def _traverse(self, X, node, prob=False):\n",
    "        if isinstance(node, Leaf):\n",
    "            if self.classifier:\n",
    "                return node.value if prob else node.value.argmax()\n",
    "            return node.value\n",
    "        if X[node.feature] <= node.threshold:\n",
    "            return self._traverse(X, node.left, prob)\n",
    "        return self._traverse(X, node.right, prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Loss functions__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss functions, minimize these\n",
    "def mse(y):\n",
    "    \"\"\"\n",
    "    Mean squared error for decision tree (ie., mean) predictions\n",
    "    \"\"\"\n",
    "    return np.mean((y - np.mean(y)) ** 2)\n",
    "\n",
    "\n",
    "def entropy(y):\n",
    "    \"\"\"\n",
    "    Entropy of a label sequence\n",
    "    \"\"\"\n",
    "    hist = np.bincount(y)\n",
    "    ps = hist / np.sum(hist)\n",
    "    return -np.sum([p * np.log2(p) for p in ps if p > 0])\n",
    "\n",
    "\n",
    "def gini(y):\n",
    "    \"\"\"\n",
    "    Gini impurity (local entropy) of a label sequence\n",
    "    \"\"\"\n",
    "    hist = np.bincount(y)\n",
    "    N = np.sum(hist)\n",
    "    return 1 - sum([(i / N) ** 2 for i in hist])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating our model\n",
    "Now that we have built our decision tree model, let's put it to the test!\n",
    "<br />\n",
    "First let's create an instance of our DecisionTree class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DecisionTree()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's extract the features and labels, remember that the labels are our last column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df[df.columns[:-1]] # features\n",
    "y = df[df.columns[-1]] # labels\n",
    "# convert to arrays\n",
    "x = np.array(x)\n",
    "y = np.array(y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can split our data into training and testing sets and fit our model to the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How should we split our data?\n",
    "# We want train and test labels as well as train and test features\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x,y, test_size=.3)\n",
    "\n",
    "# Call \".fit\" with our Decision Tree to train model on data\n",
    "dt.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we finally predict!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call \".predict()\" and supply your test data\n",
    "predictions = dt.predict(x_test)\n",
    "\n",
    "# How did we do?\n",
    "correct = 0\n",
    "incorrect = 0\n",
    "for i,j in zip(predictions, y_test):\n",
    "    if i == j:\n",
    "        correct += 1\n",
    "    else:\n",
    "        incorrect += 1\n",
    "print(\"We got {} correct and {} incorrect\".format(correct, incorrect))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'd say we did pretty good! Now let's see how that measures out as a percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "print(\"Accuracy for our decision tree: {0:.2f}%\".format(accuracy_score(y_test,predictions)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's take the average of splitting and testing the data 10 times using K-Folds cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "scores = []\n",
    "accuracy = 0\n",
    "cv = KFold(n_splits=10, random_state=42, shuffle=False)\n",
    "for train_index, test_index in cv.split(df):\n",
    "    X_train, X_test, y_train, y_test = x[train_index], x[test_index], y[train_index], y[test_index]\n",
    "    dt.fit(X_train, y_train)\n",
    "    scores.append(dt.predict(X_test))\n",
    "    accuracy += accuracy_score(y_test, dt.predict(X_test))\n",
    "print(\"10 fold accuracy for our decision tree: {0:.2f}%\".format(accuracy / len(scores) * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now I encourage you to try this with a different dataset, play around with some better visualizations, or maybe a different loss function!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
